# -*- coding: utf-8 -*-
"""HeartDiseasePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uNTjVGaFKi09z09xCb8x61YBDpFmzw8x
"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/heart.csv')

df.head()

df.shape

df.dtypes

df.info()

df.isnull().sum()

df.describe()

print(df['target'].unique())

df['target'].value_counts()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(3, 3))
sns.countplot(x="target", data=df, palette="bwr")
plt.show()

pd.crosstab(df.age,df.target).plot(kind="bar",figsize=(20,6))
plt.title('Heart Disease Frequency for Ages')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.savefig('heartDiseaseAndAges.png')
plt.show()

pd.crosstab(df.sex,df.target).plot(kind="bar",figsize=(4,3),color=['#1CA53B','#AA1111' ])
plt.title('Heart Disease Frequency for Sex')
plt.xlabel('Sex (0 = Female, 1 = Male)')
plt.xticks(rotation=0)
plt.legend(["Haven't Disease", "Have Disease"])
plt.ylabel('Frequency')
plt.show()

plt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c="red")
plt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])
plt.legend(["Disease", "Not Disease"])
plt.xlabel("Age")
plt.ylabel("Maximum Heart Rate")
plt.show()

sns.pairplot(df, y_vars=['target'])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
plt.figure(figsize=(14, 10))
for i, feature in enumerate(categorical_features, 1):
    plt.subplot(3, 3, i)
    sns.countplot(x=feature, data=df)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

corrmat = df.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(15,15))
sns.heatmap(df[top_corr_features].corr(), annot=True, cmap='RdYlGn')

corr_matrix=df.corr()

corr_matrix['target'].sort_values(ascending=False)

import matplotlib.pyplot as plt
df.hist(bins=50, figsize=(20,15))

X = df.iloc[:, [0,1,2,3,4,5,6,7,9,8,10,11,12]] # X= df.drop(columns='target', axis=1)
y = df.iloc[:, 13].values
print(X, y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, stratify=y, random_state = 42)
print(X_train.shape, X_test.shape, y_train.shape)

# Standardize the features (mean = 0, variance = 1)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model= LogisticRegression()
model.fit(X_train, y_train) #finding pattern

#Model Evaluation
X_train_pred = model.predict(X_train)
train_accuracy= accuracy_score(X_train_pred, y_train)
print(train_accuracy)

X_test_pred = model.predict(X_test)
test_accuracy= accuracy_score(X_test_pred, y_test)
print(test_accuracy)

"""Logistic regression- Confusion matrix"""

from sklearn.metrics import confusion_matrix
import seaborn as sns

cm = confusion_matrix(y_test, X_test_pred)

sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score

# Compute ROC curve and AUC
fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])
auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])

# Plot ROC curve- the greater the area under it the better the performance
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import precision_recall_curve
precision, recall, thresholds = precision_recall_curve(y_test, model.predict_proba(X_test)[:, 1])
plt.plot(recall, precision, marker='.')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.show()

# A high area under the curve represents both high recall and high precision,
# where high precision relates to a low false positive rate, and high recall
# relates to a low false negative rate

import pandas as pd
import matplotlib.pyplot as plt

X_train_df = pd.DataFrame(X_train, columns=feature_names)

feature_names = X_train_df.columns
feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': model.coef_[0]})
feature_importance['Importance'] = abs(feature_importance['Importance'])  # Use absolute values for importance

# Sort the DataFrame by importance in descending order
feature_importance.sort_values(by='Importance', ascending=False, inplace=True)

# Create a horizontal bar plot to visualize feature importance
plt.barh(feature_importance['Feature'], feature_importance['Importance'])
plt.xlabel('Feature Importance')
plt.title('Feature Importance for Logistic Regression')
plt.show()

"""PREDICTIVE SYSTEM"""

input= (41,0,1,130,204,0,0,172,0,1.4,2,0,2) # tuple -> numpyarray
array= np.asarray(input)
input_reshape= array.reshape(1,-1)
prediction= model.predict(input_reshape)

if prediction[0]== 0:
  print("No Heart Disease Dectected.")
else:
  print("Heart Disease Detected.")

"""NAIVE BAYES"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the Gaussian Naive Bayes classifier
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = nb_classifier.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Generate a classification report
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix (if you haven't already)
conf_matrix = confusion_matrix(y_test, y_pred)

# Create a heatmap for the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""SVM"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Initialize and train the SVM classifier with a linear kernel
svm_classifier = SVC(kernel='linear', C=1.0, random_state=42)
svm_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = svm_classifier.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Generate a classification report
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Calculate the confusion matrix (if you haven't already)
conf_matrix = confusion_matrix(y_test, y_pred)

# Create a heatmap for the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""SCALING FEATURES"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
print(X_train)

from sklearn.ensemble import ExtraTreesRegressor
model = ExtraTreesRegressor()
model.fit(X,y)
#An extra-trees regressor. This class implements a meta estimator that fits a number of randomized decision trees
# on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.

print(model.feature_importances_)

"""RANDOM FOREST CLASSIFIER"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train,y_train)
print(rf.score(X_test,y_test))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Make predictions on the test data
y_pred = rf.predict(X_test)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Create a heatmap for the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Random Forest')
plt.show()

"""SEQUENTIAL CLASSIFIER"""

from sklearn.metrics import confusion_matrix
from keras.models import Sequential
from keras.layers import Activation, Dense

classifier = Sequential()
classifier.add(Dense(activation = "relu", input_dim = 13, units = 8, kernel_initializer = "uniform"))
classifier.add(Dense(activation = "relu", units = 14, kernel_initializer = "uniform"))
classifier.add(Dense(activation = "sigmoid", units = 1, kernel_initializer = "uniform"))
classifier.compile(optimizer = 'adam' , loss = 'binary_crossentropy', metrics = ['accuracy'] )
classifier.fit(X_train , y_train , batch_size = 8 ,epochs = 150  )
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)
cm = confusion_matrix(y_test,y_pred)
accuracy = (cm[0][0]+cm[1][1])/(cm[0][1] + cm[1][0] +cm[0][0] +cm[1][1])
print(accuracy)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix Heatmap')
plt.show()

"""MLP CLASSIFIER"""

import pickle
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
import numpy as np
from random import shuffle
import pandas as pd

# Load your data
df = pd.read_csv('heart.csv')
X = df.iloc[:, [0,1,2,3,4,5,6,7,9,8,10,11,12]]
y = df.iloc[:, 13].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Perform PCA and scaling
pca = PCA(n_components=10).fit(X)
xtr_new = pca.transform(X_train)
xte_new = pca.transform(X_test)
scaler = StandardScaler().fit(xtr_new)
xtr_new = scaler.transform(xtr_new)
xte_new = scaler.transform(xte_new)

# Define the neural network parameters
initial_population_size = 100
population = [np.random.randint(1, 15, (3,)) for i in range(initial_population_size)]
scores = []

# Train the neural network models and perform genetic algorithm
for generation in range(initial_population_size//2):
    del scores[:]
    for sample in population:
        nn = MLPClassifier(activation='relu', solver='sgd', max_iter=800,
                           alpha=1e-5, hidden_layer_sizes=sample, random_state=85)
        nn.fit(xtr_new, y_train)
        scores.append([nn.score(xte_new, y_test), sample])

    scores.sort(key=lambda x: x[0])
    if len(scores) == 2:
        break
    # create new population
    del scores[:2]
    shuffle(scores)
    population = [model[1] for model in scores]
    new_population = []
    for index in range(len(population))[0:-1:2]:
        new_population.append(np.concatenate((population[index][:1], population[index+1][1:])))
        new_population.append(np.concatenate((population[index+1][:1], population[index][1:])))
    population = list(new_population)
    print("Generation %d out of %d: done!" % (generation + 1, initial_population_size/2))

# Assuming the best model is the first one in the sorted 'scores' list
best_model = scores[0][1]

# Create and train the final MLPClassifier model with the best parameters
final_nn = MLPClassifier(activation='relu', solver='sgd', max_iter=800, alpha=1e-5, hidden_layer_sizes=best_model, random_state=85)
final_nn.fit(xtr_new, y_train)

# Save the trained model to a file
with open('MLPmodel.pkl', 'wb') as model_file:
    pickle.dump(final_nn, model_file)

# Now, you can load the model later using the following code:
# with open('MLPmodel.pkl', 'rb') as model_file:
#     loaded_model = pickle.load(model_file)

print(scores)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Assuming 'nn' is your trained MLPClassifier model
# Assuming 'xte_new' contains the transformed test data
# Assuming 'y_test' contains the true labels for the test data

# Make predictions on the test data
y_pred = nn.predict(xte_new)

# Calculate the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Create a heatmap for the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for MLPClassifier with Genetic Algorithm')
plt.show()